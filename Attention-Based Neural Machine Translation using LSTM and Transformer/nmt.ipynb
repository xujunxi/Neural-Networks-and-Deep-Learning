{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3-nmt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "6535e300-d59c-4cf0-da8f-ed73268760f5"
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p ./content/csc421/a3/\n",
        "%cd ./content/csc421/a3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.0.0)\n",
            "Collecting Pillow==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/80/eca7a2d1a3c2dafb960f32f844d570de988e609f5fd17de92e1cf6a01b0a/Pillow-4.0.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 19.0MB/s \n",
            "\u001b[?25hCollecting olefile\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/81/e1ac43c6b45b4c5f8d9352396a14144bba52c8fec72a80f425f6a4d653ad/olefile-0.46.zip (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 53.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow, olefile\n",
            "  Building wheel for Pillow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow: filename=Pillow-4.0.0-cp37-cp37m-linux_x86_64.whl size=1007311 sha256=925775274eb74b37168b2462057f20d111706b02531bf4d4d0a41bf43baf9f7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/0a/2a/7e3391063af230fac4b5fdb4cc93adcb1d99af325b623cea03\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35416 sha256=4688316bd47f4fb89d231cfd88e04db797f63247e9b032cc5be84908ab323acc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/f4/11/bc4166107c27f07fd7bba707ffcb439619197638a1ac986df3\n",
            "Successfully built Pillow olefile\n",
            "\u001b[31mERROR: torchvision 0.9.0+cu101 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.16.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: olefile, Pillow\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-4.0.0 olefile-0.46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_lstm(l1, l2, o1, o2, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and val loss curves from LSTM runs.\n",
        "    \n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    ax[0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
        "    ax[0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
        "    ax[0].title.set_text('Train Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
        "    ax[1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
        "    ax[1].title.set_text('Val Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
        "\n",
        "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle('LSTM Performance by Dataset', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.85)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
        "\n",
        "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
        "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
        "    ax[0][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
        "    ax[0][1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
        "    ax[0][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label='ds=' + o3.data_file_name)\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='ds=' + o4.data_file_name)\n",
        "    ax[1][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o3.hidden_size))\n",
        "\n",
        "    ax[1][1].plot(range(len(l3[1])), l3[1], label='ds=' + o3.data_file_name)\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label='ds=' + o4.data_file_name)\n",
        "    ax[1][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o4.hidden_size))\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
        "\n",
        "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='hid_size=' + str(o1.hidden_size))\n",
        "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label='hid_size=' + str(o3.hidden_size))\n",
        "    ax[0][0].title.set_text('Train Loss | Dataset = ' + o1.data_file_name)\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label='hid_size=' + str(o1.hidden_size))\n",
        "    ax[0][1].plot(range(len(l3[1])), l3[1], label='hid_size=' + str(o3.hidden_size))\n",
        "    ax[0][1].title.set_text('Val Loss | Dataset = ' + o1.data_file_name)\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label='hid_size=' + str(o2.hidden_size))\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='hid_size=' + str(o4.hidden_size))\n",
        "    ax[1][0].title.set_text('Train Loss | Dataset = ' + o3.data_file_name)\n",
        "\n",
        "    ax[1][1].plot(range(len(l2[1])), l2[1], label='hid_size=' + str(o2.hidden_size))\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label='hid_size=' + str(o4.hidden_size))\n",
        "    ax[1][1].title.set_text('Val Loss | Dataset = ' + o4.data_file_name)\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "    path = \"./data/{}.txt\".format(file_name)\n",
        "    source_lines, target_lines = read_pairs(path)\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa5-onJhoSeM"
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden, encoder_last_cell = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_cell = encoder_last_cell\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1] #latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [index_to_char[int(item)] \n",
        "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden, encoder_cell = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1] #latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [index_to_char[int(item)] \n",
        "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "        ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid('off')\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden, encoder_cell = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "            decoder_cell = encoder_cell\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            \n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "    \n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "        \n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\"Validation loss has not improved in {} epochs, stopping early\".format(opts.early_stopping_patience))\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, mean_train_loss, mean_val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "        encoder = LSTMEncoder(vocab_size=vocab_size, \n",
        "                              hidden_size=opts.hidden_size, \n",
        "                              opts=opts)\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "        encoder = TransformerEncoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers,\n",
        "                                     opts=opts)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type,\n",
        "                                      opts.data_file_name)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(train_dict, val_dict, idx_dict, encoder, \n",
        "                               decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder, losses\n",
        "      \n",
        "    return encoder, decoder, losses\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe335a0d-a9a5-41bd-cfff-cdd7d5b1b2db"
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_small.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_small.txt', \n",
        "                         untar=False)\n",
        "\n",
        "data_fpath = get_file(fname='pig_latin_large.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_large.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_small.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_small.txt\n",
            "data/pig_latin_large.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_large.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw"
      },
      "source": [
        "# Part 1: Long Short-Term Memory Unit (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC"
      },
      "source": [
        "## Step 1: LSTM Cell\n",
        "Please implement the Long Short-Term Memory class defined in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOnALRQkkjDO"
      },
      "source": [
        "class MyLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        self.Wii = nn.Linear(input_size, hidden_size)\n",
        "        self.Whi = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wif = nn.Linear(input_size, hidden_size)\n",
        "        self.Whf = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wig = nn.Linear(input_size, hidden_size)\n",
        "        self.Whg = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wio = nn.Linear(input_size, hidden_size)\n",
        "        self.Who = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        \"\"\"Forward pass of the LSTM computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "            c_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "            c_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        i = torch.sigmoid(torch.add(self.Wii(x), self.Whi(h_prev)))\n",
        "        f = torch.sigmoid(torch.add(self.Wif(x), self.Whf(h_prev)))\n",
        "        g = torch.tanh(torch.add(self.Wig(x), self.Whg(h_prev)))\n",
        "        o = torch.sigmoid(torch.add(self.Wio(x), self.Who(h_prev)))\n",
        "        c_new = torch.add(torch.mul(f,c_prev), torch.mul(i, g))\n",
        "        h_new = torch.mul(o, torch.tanh(c_new))\n",
        "        return h_new, c_new"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z"
      },
      "source": [
        "## Step 2: LSTM Encoder\n",
        "Please inspect the following recurrent encoder/decoder implementations. Make sure to run the cells before proceeding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDNim2fmVJV"
      },
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.lstm = MyLSTMCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        cell = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden, cell = self.lstm(x, hidden, cell)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden, cell\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvwizYM9ma4p"
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyLSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "            cell_init: The cell states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        c_prev = cell_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev, c_prev = self.rnn(x, h_prev, c_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model comprised of recurrent encoder and decoders. \n",
        "\n",
        "First, we train on the smaller dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVuXTozTPF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f35f83-81b9-415a-c7a8-4d4374d548ce"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True,\n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\",\n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':20,\n",
        "              'batch_size':64,\n",
        "              'hidden_size':32,\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_s.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_s)\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 20                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('applying', 'applyingway')\n",
            "('luxury', 'uxurylay')\n",
            "('discovering', 'iscoveringday')\n",
            "('gratefully', 'atefullygray')\n",
            "('enjoy', 'enjoyway')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.366 | Val loss: 1.964 | Gen: ay ay intay-ay-ay-ay inay intay-ay-ay\n",
            "Epoch:   1 | Train loss: 1.866 | Val loss: 1.806 | Gen: ingay artay ingesay-ingay-ingway ingway inglay-ingay\n",
            "Epoch:   2 | Train loss: 1.711 | Val loss: 1.682 | Gen: onay arday illlllway-ingway ingway olingay-ay-ay\n",
            "Epoch:   3 | Train loss: 1.575 | Val loss: 1.625 | Gen: edway ay-ay olillway-atingway ilway ollway-ay\n",
            "Epoch:   4 | Train loss: 1.477 | Val loss: 1.531 | Gen: edway artay olillllway-ybay isway illway\n",
            "Epoch:   5 | Train loss: 1.374 | Val loss: 1.510 | Gen: etay ay oondillway-ybay isway olingsay\n",
            "Epoch:   6 | Train loss: 1.297 | Val loss: 1.438 | Gen: edhay arway oodilicay-orstay isway oodilyway\n",
            "Epoch:   7 | Train loss: 1.216 | Val loss: 1.359 | Gen: etay ay-ay oodilicay-ationsway isway incousay\n",
            "Epoch:   8 | Train loss: 1.147 | Val loss: 1.280 | Gen: ethay arway olindingday isway olinday\n",
            "Epoch:   9 | Train loss: 1.086 | Val loss: 1.264 | Gen: ethay arway olivedicay-idway isway olindway\n",
            "Epoch:  10 | Train loss: 1.029 | Val loss: 1.236 | Gen: ethay arway olivedingday isway olingray\n",
            "Epoch:  11 | Train loss: 0.980 | Val loss: 1.194 | Gen: ethay arway olivedicationway isway oligray\n",
            "Epoch:  12 | Train loss: 0.935 | Val loss: 1.150 | Gen: ethay arway olivedingsday isway oligray\n",
            "Epoch:  13 | Train loss: 0.885 | Val loss: 1.123 | Gen: ethay arway oonindicationway isway oligray\n",
            "Epoch:  14 | Train loss: 0.851 | Val loss: 1.163 | Gen: ethay arway ondingray-oidedway isway oligray\n",
            "Epoch:  15 | Train loss: 0.831 | Val loss: 1.106 | Gen: ethay arway oolivedsticay isway oligray\n",
            "Epoch:  16 | Train loss: 0.795 | Val loss: 1.088 | Gen: ethay arway ondirancingday isway ortionway\n",
            "Epoch:  17 | Train loss: 0.764 | Val loss: 1.034 | Gen: ethay arway ondiringstay-idway isway ortionway\n",
            "Epoch:  18 | Train loss: 0.728 | Val loss: 1.029 | Gen: ethay arway ondingibledway isway orkingway\n",
            "Epoch:  19 | Train loss: 0.700 | Val loss: 1.047 | Gen: ethay arway oninintideway isway ortionway\n",
            "Epoch:  20 | Train loss: 0.682 | Val loss: 0.987 | Gen: ethay arway oninitingday-idway isway orkingway\n",
            "Epoch:  21 | Train loss: 0.661 | Val loss: 1.060 | Gen: ethay arway oniringsday-idway isway orkingway\n",
            "Epoch:  22 | Train loss: 0.653 | Val loss: 1.038 | Gen: ethay arway onirinttingday isway ortionway\n",
            "Epoch:  23 | Train loss: 0.625 | Val loss: 0.950 | Gen: ethay arway onoritindfay-idway isway orkingway\n",
            "Epoch:  24 | Train loss: 0.592 | Val loss: 0.983 | Gen: ethay arway onitioningday isway orkingway\n",
            "Epoch:  25 | Train loss: 0.577 | Val loss: 0.970 | Gen: ethay arway oninotiondday isway orkingway\n",
            "Epoch:  26 | Train loss: 0.560 | Val loss: 0.964 | Gen: ethay arway ondiringstray isway orkingway\n",
            "Epoch:  27 | Train loss: 0.540 | Val loss: 0.931 | Gen: ethay arway oninitingdecay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.518 | Val loss: 0.966 | Gen: ethay arway onitioningday isway orkingway\n",
            "Epoch:  29 | Train loss: 0.517 | Val loss: 0.956 | Gen: ethay arway oninitiondgay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.505 | Val loss: 0.955 | Gen: ethay arway onitiondicatyday isway orkingway\n",
            "Epoch:  31 | Train loss: 0.495 | Val loss: 0.970 | Gen: ethay arway onintiontidgay isway ortilay\n",
            "Epoch:  32 | Train loss: 0.491 | Val loss: 0.944 | Gen: ethay arway onitioningdray isway orkingway\n",
            "Epoch:  33 | Train loss: 0.465 | Val loss: 0.948 | Gen: ethay arway onsionditidgay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.445 | Val loss: 0.979 | Gen: ethay arway oninitionday isway orkingway\n",
            "Epoch:  35 | Train loss: 0.431 | Val loss: 0.935 | Gen: ethay arway oninitiondgay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.411 | Val loss: 0.931 | Gen: ethay arway onintiondtay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.394 | Val loss: 0.950 | Gen: ethay arway onditiontay-otrawday isway orkingway\n",
            "Epoch:  38 | Train loss: 0.390 | Val loss: 0.910 | Gen: ethay arway onnirtiondfay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.381 | Val loss: 0.913 | Gen: ethay arway ondicintiongay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.380 | Val loss: 0.935 | Gen: ethay arway ondisiontacyday isway orkingway\n",
            "Epoch:  41 | Train loss: 0.387 | Val loss: 0.927 | Gen: ethay ariway onnitiondicateway isway orkingway\n",
            "Epoch:  42 | Train loss: 0.364 | Val loss: 0.899 | Gen: ethay arway ondisiotioncay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.346 | Val loss: 0.911 | Gen: ethay ariway onndicintidayway isway orkingway\n",
            "Epoch:  44 | Train loss: 0.331 | Val loss: 0.933 | Gen: ethay arvay onnitiondicateway isway orkingway\n",
            "Epoch:  45 | Train loss: 0.319 | Val loss: 0.905 | Gen: ethay arvay onnitiondicateway isway orkingway\n",
            "Epoch:  46 | Train loss: 0.309 | Val loss: 0.915 | Gen: ethay arvay onncividtandiway isway orkingway\n",
            "Epoch:  47 | Train loss: 0.302 | Val loss: 0.921 | Gen: ethay ariway onndiciedtay-ifray isway orkingway\n",
            "Epoch:  48 | Train loss: 0.300 | Val loss: 0.949 | Gen: ethay ariway ondivinttay-ortway isway orkingway\n",
            "Epoch:  49 | Train loss: 0.301 | Val loss: 0.964 | Gen: ethay arvay ondicintiongay isway orkingway\n",
            "Obtained lowest validation loss of: 0.899235216731375\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arvay ondicintiongay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mR97V_NtER6"
      },
      "source": [
        "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n",
        "\n",
        "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a rough and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b092e4-4204-4956-e8ea-cd287dbf553e"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large',\n",
        "              'cuda':True,\n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\",\n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':512,\n",
        "              'hidden_size':32,\n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_l.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_l)\n",
        "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('initiation', 'initiationway')\n",
            "('inquisitiveness', 'inquisitivenessway')\n",
            "('quiz', 'uizqay')\n",
            "('ata', 'ataway')\n",
            "('clock', 'ockclay')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.395 | Val loss: 2.078 | Gen: eray eray-intay ontay-intay-eray etay ontay-intay\n",
            "Epoch:   1 | Train loss: 1.924 | Val loss: 1.891 | Gen: ertay ay-ertay ontay-inserteray onsay onsay-insay\n",
            "Epoch:   2 | Train loss: 1.739 | Val loss: 1.739 | Gen: erway ay-ay-ay ontay-onway-ay-ay onway ontay-onway\n",
            "Epoch:   3 | Train loss: 1.592 | Val loss: 1.663 | Gen: ertay ay-ay-ay ontay-ongay-onsway inway otay-onsay\n",
            "Epoch:   4 | Train loss: 1.474 | Val loss: 1.624 | Gen: etay ationway ongongray-onday-ay isway ongay-onday\n",
            "Epoch:   5 | Train loss: 1.384 | Val loss: 1.568 | Gen: etay away-away ondinay-onday-anday isway onday-onway\n",
            "Epoch:   6 | Train loss: 1.305 | Val loss: 1.455 | Gen: etay away-away ondingay-ontay-awlay isway ondgay-onday\n",
            "Epoch:   7 | Train loss: 1.217 | Val loss: 1.422 | Gen: etay away ontoonway-onway isway onghay-ybay\n",
            "Epoch:   8 | Train loss: 1.139 | Val loss: 1.344 | Gen: etay arway ondingay-othay isway orkay-inway\n",
            "Epoch:   9 | Train loss: 1.068 | Val loss: 1.345 | Gen: etay away ondingay-otay-entay isway onghay-anway\n",
            "Epoch:  10 | Train loss: 1.014 | Val loss: 1.339 | Gen: etay airay ondingay-intay isway oupray-ybay\n",
            "Epoch:  11 | Train loss: 0.959 | Val loss: 1.212 | Gen: ethay aiway ondingay-intay isway okerghay\n",
            "Epoch:  12 | Train loss: 0.909 | Val loss: 1.190 | Gen: ethay airway onthondicatioway isway oupreryway\n",
            "Epoch:  13 | Train loss: 0.850 | Val loss: 1.146 | Gen: ethay airay ondintay-intay isway ouprentay\n",
            "Epoch:  14 | Train loss: 0.812 | Val loss: 1.128 | Gen: ethay airay ondinghontay-anway isway onghhbray\n",
            "Epoch:  15 | Train loss: 0.803 | Val loss: 1.093 | Gen: ethay iaray ondingay-othcay isway okerghay\n",
            "Epoch:  16 | Train loss: 0.772 | Val loss: 1.067 | Gen: ethay airway ondicationdray isway ounphteway\n",
            "Epoch:  17 | Train loss: 0.726 | Val loss: 1.026 | Gen: ethay iaray ondinghay-otencay isway okenghay\n",
            "Epoch:  18 | Train loss: 0.681 | Val loss: 1.097 | Gen: ethay airway ondintay-ondray isway onkingway\n",
            "Epoch:  19 | Train loss: 0.672 | Val loss: 0.996 | Gen: ethay airay ondingay-othenay isway ounshichay\n",
            "Epoch:  20 | Train loss: 0.635 | Val loss: 0.969 | Gen: ethay airay ondinghabilcay isway onkigray\n",
            "Epoch:  21 | Train loss: 0.610 | Val loss: 0.977 | Gen: ethay iarway ondingabilitay isway onkighay\n",
            "Epoch:  22 | Train loss: 0.589 | Val loss: 0.976 | Gen: ethay airay ondicationingay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.582 | Val loss: 0.975 | Gen: ethay airway ondingchay-inway isway onkray-imway\n",
            "Epoch:  24 | Train loss: 0.575 | Val loss: 0.960 | Gen: ethay airay ondingay-intay-away isway orkingway\n",
            "Epoch:  25 | Train loss: 0.569 | Val loss: 0.950 | Gen: ethay airway ondinginatiocay isway ounshiay\n",
            "Epoch:  26 | Train loss: 0.541 | Val loss: 0.946 | Gen: ethay airway ondicationginway isway okerghay\n",
            "Epoch:  27 | Train loss: 0.524 | Val loss: 0.922 | Gen: ethay airway ondintay-ondray isway onkray-imway\n",
            "Epoch:  28 | Train loss: 0.501 | Val loss: 0.939 | Gen: ethay airway ondingay-indtay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.487 | Val loss: 0.886 | Gen: ethay airway ondinginatioday isway orkingway\n",
            "Epoch:  30 | Train loss: 0.473 | Val loss: 0.938 | Gen: ethay airway ondintonicay-orway isway onkray-imway\n",
            "Epoch:  31 | Train loss: 0.477 | Val loss: 0.914 | Gen: ethay iraway ondingay-intay-away isway orkingway\n",
            "Epoch:  32 | Train loss: 0.457 | Val loss: 0.896 | Gen: ethay airway ondingabilay-anway isway oknhay-emway\n",
            "Epoch:  33 | Train loss: 0.447 | Val loss: 0.874 | Gen: ethay airway ondingatiochay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.429 | Val loss: 0.865 | Gen: ethay iraway ondingichay-onsay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.427 | Val loss: 0.944 | Gen: ethay airway ondingatiodray isway orkingway\n",
            "Epoch:  36 | Train loss: 0.429 | Val loss: 0.868 | Gen: ethay airway ondingichay-onsay isway onkirway\n",
            "Epoch:  37 | Train loss: 0.407 | Val loss: 0.842 | Gen: ethay airway ondicationingay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.399 | Val loss: 0.842 | Gen: ehay airway ondingiationcay isway onkirway\n",
            "Epoch:  39 | Train loss: 0.394 | Val loss: 0.890 | Gen: ethay airway ondingabilitay isway onkray-imway\n",
            "Epoch:  40 | Train loss: 0.389 | Val loss: 0.882 | Gen: ethay airway ondingiablishay isway oknshay-away\n",
            "Epoch:  41 | Train loss: 0.392 | Val loss: 0.854 | Gen: ehay iraway ondingichay-onway isway orkingway\n",
            "Epoch:  42 | Train loss: 0.376 | Val loss: 0.855 | Gen: ethay airway ondingichay-onsay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.361 | Val loss: 0.819 | Gen: ethay iraway ondingichay-onsay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.353 | Val loss: 0.847 | Gen: ethay airway ondingichay-onsay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.348 | Val loss: 0.791 | Gen: ethay airway ondingichay-anway isway onkirway\n",
            "Epoch:  46 | Train loss: 0.329 | Val loss: 0.799 | Gen: ethay airway ondingichay-anway isway oxbriskay\n",
            "Epoch:  47 | Train loss: 0.323 | Val loss: 0.782 | Gen: ethay airway ondingichay-onsay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.320 | Val loss: 0.836 | Gen: ethay airway ondingationcay isway ojinstway\n",
            "Epoch:  49 | Train loss: 0.316 | Val loss: 0.769 | Gen: ethay airway ondingichay-onway isway orkingway\n",
            "Obtained lowest validation loss of: 0.7693998726317659\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway ondingichay-onway isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HsZ6EItc56"
      },
      "source": [
        "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Consider if there are significant differences in the validation performance of each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "Qyk_9-Fwtekj",
        "outputId": "8af37b7c-061d-4606-b0bd-ee96b67d402f"
      },
      "source": [
        "save_loss_comparison_lstm(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, 'lstm')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt"
      },
      "source": [
        "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3f9551-040a-44b1-eddb-949a1b2133cf"
      },
      "source": [
        "best_encoder = rnn_encode_l # Replace with rnn_losses_s or rnn_losses_l\n",
        "best_decoder = rnn_decoder_l # etc.\n",
        "best_args = rnn_args_l\n",
        "\n",
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, best_encoder, best_decoder, None, best_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway ondingichay-onway isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XATAbNIuMnZ8",
        "outputId": "f7bf00c4-8ecc-4d2d-afb2-7cd11434fddd"
      },
      "source": [
        "TEST_SENTENCE_2 = 'this beautiful shopping mall'\n",
        "translated_2 = translate_sentence(TEST_SENTENCE_2, best_encoder, best_decoder, None, best_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE_2, translated_2))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthis beautiful shopping mall \n",
            "translated:\tisthay eautuinsway oppingspay allmay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYXD14CUN7ks",
        "outputId": "35f331f1-7809-4e5d-a37a-f6ae7b23250b"
      },
      "source": [
        "TEST_SENTENCE_3 = 'extremely happy ending'\n",
        "translated_3 = translate_sentence(TEST_SENTENCE_3, best_encoder, best_decoder, None, best_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE_3, translated_3))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\textremely happy ending \n",
            "translated:\texterledgway apphyay endingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt5caOXmOM5t",
        "outputId": "3fa0f2f0-3f3a-4adb-cda6-1b1819bbe2fe"
      },
      "source": [
        "TEST_SENTENCE_4 = 'go to school now'\n",
        "translated_4 = translate_sentence(TEST_SENTENCE_4, best_encoder, best_decoder, None, best_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE_4, translated_4))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tgo to school now \n",
            "translated:\togay otay oolcepay ownay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq"
      },
      "source": [
        "# Part 2: Additive Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w"
      },
      "source": [
        "## Step 1: Additive Attention\n",
        "Already implemented the additive attention mechanism. Write down the mathematical expression for $\\tilde{\\alpha}_i^{(t)}, \\alpha_i^{(t)}, c_t$ as a function of $W_1, W_2, b_1, b_2, Q_t, K_i$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdewEVSMo5jJ"
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1) #the seq_len dimension\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys) #-1: add one dimension in middle, expand_as: make it same shape as keys\n",
        "        #print(\"expanded_queries:\", expanded_queries.size())\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2) #concat along the dimension 2\n",
        "        #print(\"concat_inputs:\", concat_inputs.size())\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        #print(\"unnormalized_attention:\",unnormalized_attention.size())\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2,1), values)\n",
        "        return context, attention_weights\n",
        "      "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ"
      },
      "source": [
        "## Step 2: RNN Additive Attention Decoder\n",
        "We will now implement a recurrent decoder that makes use of the additive attention mechanism. Read the description in the assignment worksheet and complete the following implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJaABkXrpJSw"
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyLSTMCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "            cell_init: The final cell states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        c_prev = cell_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            embed_current = embed[:,i,:]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(h_prev, annotations, annotations)  # batch_size x 1 x hidden_size\n",
        "            embed_and_context = torch.cat([embed_current, context.squeeze(1)], dim=1)  # batch_size x (2*hidden_size)\n",
        "            h_prev, c_prev = self.rnn(embed_and_context, h_prev, c_prev)  # batch_size x hidden_size            \n",
        "            \n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model that uses a recurrent encoder, and a recurrent decoder that has an additive attention component. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke6t6rCezpZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e645782-3f03-4d25-e280-700ab3cfc4c5"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "rnn_attn_args = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005,\n",
        "              'lr_decay':0.99,\n",
        "              'early_stopping_patience':10,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':64, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "rnn_attn_args.update(args_dict)\n",
        "\n",
        "print_opts(rnn_attn_args)\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('applying', 'applyingway')\n",
            "('luxury', 'uxurylay')\n",
            "('discovering', 'iscoveringday')\n",
            "('gratefully', 'atefullygray')\n",
            "('enjoy', 'enjoyway')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.098 | Val loss: 1.783 | Gen: illay-ay ay-ay ingay-ingay-ingay illay illay-onday\n",
            "Epoch:   1 | Train loss: 1.573 | Val loss: 1.571 | Gen: etay anday indiondionday isway illway-indway\n",
            "Epoch:   2 | Train loss: 1.319 | Val loss: 1.359 | Gen: etetway aricay ondiondionday isway oonday-ay\n",
            "Epoch:   3 | Train loss: 1.095 | Val loss: 1.224 | Gen: etay-ay aricay ondiondionday isway oourdway\n",
            "Epoch:   4 | Train loss: 0.944 | Val loss: 1.164 | Gen: ethay arifay onsicitinginway isway ootingnay\n",
            "Epoch:   5 | Train loss: 0.824 | Val loss: 1.107 | Gen: ethay ariyway ondicitingway isway orthingnay\n",
            "Epoch:   6 | Train loss: 0.725 | Val loss: 1.037 | Gen: ighay aichay ondicingingway isway ordgay-onday\n",
            "Epoch:   7 | Train loss: 0.646 | Val loss: 1.120 | Gen: aty-atway ariway ondistingsay isway orway-othay\n",
            "Epoch:   8 | Train loss: 0.622 | Val loss: 0.889 | Gen: ehtay airway ondintingway isway ordingway\n",
            "Epoch:   9 | Train loss: 0.480 | Val loss: 0.747 | Gen: ateway airway ondicingingway isway orway-ingway\n",
            "Epoch:  10 | Train loss: 0.376 | Val loss: 0.701 | Gen: ehay airway ondicinitingway isway orway-ingnay\n",
            "Epoch:  11 | Train loss: 0.303 | Val loss: 0.777 | Gen: ehtay airway ondicinitingway isway orkingway\n",
            "Epoch:  12 | Train loss: 0.276 | Val loss: 0.626 | Gen: ethay airway ondicinitinway isway oringnay\n",
            "Epoch:  13 | Train loss: 0.216 | Val loss: 0.547 | Gen: ehtay airway ondicitioncyway isway orkingay\n",
            "Epoch:  14 | Train loss: 0.195 | Val loss: 0.616 | Gen: ethay airway ondiitionciday isway orkingway\n",
            "Epoch:  15 | Train loss: 0.188 | Val loss: 0.548 | Gen: ehtay airway ondiiciningway isway orkingray\n",
            "Epoch:  16 | Train loss: 0.157 | Val loss: 0.560 | Gen: ethay airway onditiiningcay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.148 | Val loss: 0.545 | Gen: ehtay airway ondiicingpay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.144 | Val loss: 0.587 | Gen: ethay airway onditioncingway isway orkingway\n",
            "Epoch:  19 | Train loss: 0.116 | Val loss: 0.471 | Gen: ethay airway onditioninchay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.094 | Val loss: 0.465 | Gen: ethay airway onditionichinway isway orkingnay\n",
            "Epoch:  21 | Train loss: 0.074 | Val loss: 0.450 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.057 | Val loss: 0.400 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.044 | Val loss: 0.394 | Gen: ethay airway onditionichinway isway orkingway\n",
            "Epoch:  24 | Train loss: 0.038 | Val loss: 0.405 | Gen: ethay airway onditioncingway isway orkingway\n",
            "Epoch:  25 | Train loss: 0.034 | Val loss: 0.388 | Gen: ethay airway onditicingday isway orkingway\n",
            "Epoch:  26 | Train loss: 0.027 | Val loss: 0.360 | Gen: ethay airway onditicingday isway orkingway\n",
            "Epoch:  27 | Train loss: 0.022 | Val loss: 0.348 | Gen: ethay airway onditicingcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.018 | Val loss: 0.337 | Gen: ethay airway onditioncingway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.017 | Val loss: 0.336 | Gen: ethay airway onditioncingway isway orkingway\n",
            "Epoch:  30 | Train loss: 0.015 | Val loss: 0.376 | Gen: ethay airway onditionicnway isway orkingway\n",
            "Epoch:  31 | Train loss: 0.016 | Val loss: 0.342 | Gen: ethay airway onditicingcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.013 | Val loss: 0.333 | Gen: ethay airway onditicingcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.011 | Val loss: 0.326 | Gen: ethay airway onditicingcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.068 | Val loss: 0.914 | Gen: ethay airway ondicitlgndway isway orkngsway\n",
            "Epoch:  35 | Train loss: 0.296 | Val loss: 1.003 | Gen: etheway airway onditionchnay isway orkighway\n",
            "Epoch:  36 | Train loss: 0.372 | Val loss: 0.635 | Gen: ehtay airway onditiongingway isway orkway-ingway\n",
            "Epoch:  37 | Train loss: 0.205 | Val loss: 0.471 | Gen: ehtay airway ondititionchay isway orkway\n",
            "Epoch:  38 | Train loss: 0.118 | Val loss: 0.430 | Gen: ehtay airway ondititioncay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.072 | Val loss: 0.332 | Gen: ethay airway ondititionchay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.042 | Val loss: 0.328 | Gen: ethay airway ondititionchay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.029 | Val loss: 0.303 | Gen: ethay airway ondititionchay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.021 | Val loss: 0.309 | Gen: ethay airway onditiontincay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.017 | Val loss: 0.312 | Gen: ethay airway onditiontincay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.014 | Val loss: 0.291 | Gen: ethay airway onditiontincay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.012 | Val loss: 0.294 | Gen: ethay airway ondititionchay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.011 | Val loss: 0.291 | Gen: ethay airway onditicingcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.009 | Val loss: 0.290 | Gen: ethay airway onditiontincay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.008 | Val loss: 0.286 | Gen: ethay airway onditiontincay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.007 | Val loss: 0.285 | Gen: ethay airway onditicingcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.2854443837026007\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditicingcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNVKbLc0ACj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "149c75ba-8b32-418c-9e3d-4b83aa8f8e98"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditicingcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw_GOIvzo1ix"
      },
      "source": [
        "# Part 3: Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-"
      },
      "source": [
        "## Step 1: Implement Dot-Product Attention\n",
        "Implement the scaled dot product attention module described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3swZR_3UfaA",
        "outputId": "df32f782-9a20-4b60-a46d-fa9a80e26d5c"
      },
      "source": [
        "bbb = torch.randint(1,5,(2,4))\n",
        "aaa = torch.rsqrt(torch.tensor(64, dtype= torch.float))\n",
        "print(bbb)\n",
        "print(aaa)\n",
        "print(torch.mul(bbb,aaa))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 1, 3, 2],\n",
            "        [2, 3, 1, 1]])\n",
            "tensor(0.1250)\n",
            "tensor([[0.1250, 0.1250, 0.3750, 0.2500],\n",
            "        [0.2500, 0.3750, 0.1250, 0.1250]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_j3oY3hqsJQ"
      },
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size) #WQ\n",
        "        self.K = nn.Linear(hidden_size, hidden_size) #WK\n",
        "        self.V = nn.Linear(hidden_size, hidden_size) #WV\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float)) #output = 1/(sqrt(input))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = queries.size(0)\n",
        "        q = self.Q(queries) #batch_size x (k) x hidden_size\n",
        "        k = self.K(keys) #batch_size x seq_len x hidden_size\n",
        "        v = self.V(values) #batch_size x seq_len x hidden_size\n",
        "        unnormalized_attention = torch.bmm(k, torch.transpose(q,2,1)) #batch_size x seq_len x (k)\n",
        "        attention_weights = self.softmax(torch.mul(unnormalized_attention, self.scaling_factor)) #batch_size x seq_len x (k)\n",
        "        context = torch.bmm(torch.transpose(attention_weights,2,1), v) #batch_size x (k) x hidden_size\n",
        "        return context, attention_weights"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113"
      },
      "source": [
        "## Step 2: Implement Causal Dot-Product Attention\n",
        "Now implement the scaled causal dot product described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmKXG6gOr8kJ",
        "outputId": "345bd64a-7434-4440-cb9f-0883fcf77b6f"
      },
      "source": [
        "fff = torch.randn(4, 7)\n",
        "torch.tril(torch.tensor(-1e7).expand_as(fff), diagonal=-1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[        0.,         0.,         0.,         0.,         0.,         0.,\n",
              "                 0.],\n",
              "        [-10000000.,         0.,         0.,         0.,         0.,         0.,\n",
              "                 0.],\n",
              "        [-10000000., -10000000.,         0.,         0.,         0.,         0.,\n",
              "                 0.],\n",
              "        [-10000000., -10000000., -10000000.,         0.,         0.,         0.,\n",
              "                 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dATS_h1oBrsW",
        "outputId": "cb2ca189-b963-41ff-a2e2-30cd2cca880d"
      },
      "source": [
        "bbb = torch.randint(1,50,(5,6))\n",
        "print(bbb)\n",
        "ccc = bbb[:, 0::2]\n",
        "print(ccc)\n",
        "ddd = bbb[:, 1::2]\n",
        "print(ddd)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[17, 25, 19, 18, 38, 33],\n",
            "        [15, 22, 34, 32, 34,  9],\n",
            "        [ 8, 27, 47, 31, 39, 33],\n",
            "        [48,  1, 32, 41, 21,  2],\n",
            "        [46, 25, 38, 43, 44, 11]])\n",
            "tensor([[17, 19, 38],\n",
            "        [15, 34, 34],\n",
            "        [ 8, 47, 39],\n",
            "        [48, 32, 21],\n",
            "        [46, 38, 44]])\n",
            "tensor([[25, 18, 33],\n",
            "        [22, 32,  9],\n",
            "        [27, 31, 33],\n",
            "        [ 1, 41,  2],\n",
            "        [25, 43, 11]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xzgfqxT-_Iq",
        "outputId": "d381c481-7779-43e8-aa81-ab46f143ce3e"
      },
      "source": [
        "kkk = torch.arange(64//2)[None, ...]\n",
        "ggg = (2*kkk).float()/(64)\n",
        "vvv = torch.arange(1000)[..., None] / (10000**ggg)\n",
        "vvv.size()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovigzQffrKqj"
      },
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size) #WQ\n",
        "        self.K = nn.Linear(hidden_size, hidden_size) #WK\n",
        "        self.V = nn.Linear(hidden_size, hidden_size) #WV\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float)) #output = 1/(sqrt(input))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = queries.size(0)\n",
        "        q = self.Q(queries) #batch_size x (k) x hidden_size\n",
        "        k = self.K(keys) #batch_size x seq_len x hidden_size\n",
        "        v = self.V(values) #batch_size x seq_len x hidden_size\n",
        "        unnormalized_attention = torch.bmm(k, torch.transpose(q,2,1)) #batch_size x seq_len x (k)\n",
        "        mask = torch.add(torch.tril(self.neg_inf.expand_as(unnormalized_attention), diagonal=-1).cuda(), unnormalized_attention) #batch_size x seq_len x (k)\n",
        "        attention_weights = self.softmax(torch.mul(mask, self.scaling_factor)) #batch_size x seq_len x (k)\n",
        "        context = torch.bmm(torch.transpose(attention_weights,2,1), v) #batch_size x (k) x hidden_size\n",
        "        return context, attention_weights"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi"
      },
      "source": [
        "## Step 3: Transformer Encoder\n",
        "Complete the following transformer encoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3B-fWsarlVk"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "            None: Used to conform to standard encoder return signature.        \n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](annotations, annotations, annotations)  # batch_size x seq_len x hidden_size; annotations for all queries, keys, values\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "\n",
        "        # Transformer encoder does not have a last hidden or cell layer. \n",
        "        return annotations, None, None\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None] #pos: [0],[1],[2],[3]...[999] stack vertical\n",
        "        dim_indices = torch.arange(self.hidden_size//2)[None, ...] #i: [0],[1],[2],[3]...[hidden size/2] stack horizontal\n",
        "        exponents = (2*dim_indices).float()/(self.hidden_size) #2i/d_model\n",
        "        trig_args = pos_indices / (10000**exponents) #size: 1000x (hidden size/2)\n",
        "        sin_terms = torch.sin(trig_args) #size: 1000x (hidden size/2)\n",
        "        cos_terms = torch.cos(trig_args) #size: 1000x (hidden size/2)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms #column[0], column[2], column[4]...\n",
        "        pos_encodings[:, 1::2] = cos_terms #column[1], column[3], column[5]...\n",
        "\n",
        "        if self.opts.cuda:\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1hDi020rT36"
      },
      "source": [
        "## Step 4: Transformer Decoder\n",
        "Complete the following transformer decoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyvTZFxtrvc6"
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size) #probability over vocab\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "            cell_init: Not used in transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        embed = embed + self.positional_encodings[:seq_len]\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations) # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list) #stack weights for all decoder blocks\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list) #stack weights for all decoder blocks\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb"
      },
      "source": [
        "\n",
        "## Step 5: Training and analysis\n",
        "Now, train the following language model that's comprised of a (simplified) transformer encoder and transformer decoder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqTp-eCPuuFO"
      },
      "source": [
        "First, we train our smaller model on the small dataset. Use this model to answer Question 4 in the handout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk8e4KSnuZ8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a8b447-3a4e-4b77-eb35-6243a8941b5d"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans32_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 100,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 64,\n",
        "              'hidden_size': 32,\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans32_args_s.update(args_dict)\n",
        "print_opts(trans32_args_s)\n",
        "\n",
        "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('applying', 'applyingway')\n",
            "('luxury', 'uxurylay')\n",
            "('discovering', 'iscoveringday')\n",
            "('gratefully', 'atefullygray')\n",
            "('enjoy', 'enjoyway')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.973 | Val loss: 2.382 | Gen: aylayay ay iosiosiioay-indioay ilaylaylaylaylaylayy ieiay-inday\n",
            "Epoch:   1 | Train loss: 2.174 | Val loss: 1.998 | Gen: aylay ay indooseindoay ilaylay indeioay-ay-ay\n",
            "Epoch:   2 | Train loss: 1.911 | Val loss: 1.825 | Gen: eteway ay oooooinindondoncay isway oneoay-in-ay\n",
            "Epoch:   3 | Train loss: 1.743 | Val loss: 1.707 | Gen: eteway away oooonindondonday isway ongray-ay-ay\n",
            "Epoch:   4 | Train loss: 1.608 | Val loss: 1.624 | Gen: eteway away ontinincincintinay isay ongray-ay-ay\n",
            "Epoch:   5 | Train loss: 1.502 | Val loss: 1.569 | Gen: eteway away ontinincay-ontitiona isay onay-ingray\n",
            "Epoch:   6 | Train loss: 1.408 | Val loss: 1.547 | Gen: eteway away intionay isay ingray-ingay\n",
            "Epoch:   7 | Train loss: 1.319 | Val loss: 1.603 | Gen: etay ay incinincay-ontitiont isay ingray-ingay\n",
            "Epoch:   8 | Train loss: 1.288 | Val loss: 1.536 | Gen: ethay iray indinindinay-ontitit isay ingray-ingay\n",
            "Epoch:   9 | Train loss: 1.201 | Val loss: 1.388 | Gen: etay iaray ondindindongnday-ont isay ingray-ingngway\n",
            "Epoch:  10 | Train loss: 1.131 | Val loss: 1.484 | Gen: ethay iraray ondingingngndingngng isay oningingrway-angngng\n",
            "Epoch:  11 | Train loss: 1.063 | Val loss: 1.416 | Gen: ethay iraray ondindingngndonday-o isway oningingrway\n",
            "Epoch:  12 | Train loss: 1.006 | Val loss: 1.357 | Gen: eteteway iraray onondindongndonday-o isway onoringrway\n",
            "Epoch:  13 | Train loss: 0.953 | Val loss: 1.307 | Gen: etetehay iraray ondindondongndonay-o isway oningray-angnay\n",
            "Epoch:  14 | Train loss: 0.918 | Val loss: 1.326 | Gen: ethay iraway onondioncay-ondiondi isway onoringway\n",
            "Epoch:  15 | Train loss: 0.888 | Val loss: 1.320 | Gen: etehay ary ondincincongndondond isway oringway-oway\n",
            "Epoch:  16 | Train loss: 0.880 | Val loss: 1.264 | Gen: ethay arway ondingangngngntay-on isway oringway-oway\n",
            "Epoch:  17 | Train loss: 0.845 | Val loss: 1.281 | Gen: ethay arway ondioncay-ondgtition isway ororingway\n",
            "Epoch:  18 | Train loss: 0.793 | Val loss: 1.372 | Gen: ethay arway onondoncangncay-onti isway oooringway\n",
            "Epoch:  19 | Train loss: 0.758 | Val loss: 1.362 | Gen: ethay arway onondoncay-ondingonc isway ooringway\n",
            "Epoch:  20 | Train loss: 0.726 | Val loss: 1.240 | Gen: ethay arirway onondiongngsticay-on isway ooringway\n",
            "Epoch:  21 | Train loss: 0.712 | Val loss: 1.173 | Gen: ethay arway onotiongcay-ondition isway okwingway\n",
            "Epoch:  22 | Train loss: 0.669 | Val loss: 1.195 | Gen: ethay arirway onondiongcay-ontiong isway ooringway\n",
            "Epoch:  23 | Train loss: 0.648 | Val loss: 1.138 | Gen: ethay arirway ondioncationgnay isway okwingway\n",
            "Epoch:  24 | Train loss: 0.619 | Val loss: 1.087 | Gen: ethay arirway ondiongcay-otingngna isway origwingway\n",
            "Epoch:  25 | Train loss: 0.592 | Val loss: 1.153 | Gen: ethay arirway ondioncatingway-onan isway okwingway\n",
            "Epoch:  26 | Train loss: 0.578 | Val loss: 1.043 | Gen: ethay arirway ondioncatingway-onan isway oringway\n",
            "Epoch:  27 | Train loss: 0.560 | Val loss: 1.067 | Gen: ethehay ararway ondioncay-otinationa isway oringway\n",
            "Epoch:  28 | Train loss: 0.555 | Val loss: 1.004 | Gen: ethay ararway ondioncangway-onday isway owingway\n",
            "Epoch:  29 | Train loss: 0.538 | Val loss: 1.087 | Gen: ehethay ararway ondicingwatingnay isway okwingway\n",
            "Epoch:  30 | Train loss: 0.535 | Val loss: 1.031 | Gen: ethay ararway ondintingway isway owingway\n",
            "Epoch:  31 | Train loss: 0.527 | Val loss: 1.035 | Gen: ethay arirway onditingcay-onditing isway okwingway\n",
            "Epoch:  32 | Train loss: 0.489 | Val loss: 0.954 | Gen: ethay ariray onditingway-ondinay isway owingway\n",
            "Epoch:  33 | Train loss: 0.462 | Val loss: 0.973 | Gen: ethay ararway onditingway-ondingin isway okwingway\n",
            "Epoch:  34 | Train loss: 0.437 | Val loss: 0.964 | Gen: ethay ararway onditingway-ondingin isway okwingway\n",
            "Epoch:  35 | Train loss: 0.420 | Val loss: 0.965 | Gen: ethay ararway onditingway isway okwingway\n",
            "Epoch:  36 | Train loss: 0.405 | Val loss: 0.976 | Gen: ethay ararway onditingway isway okwingway\n",
            "Epoch:  37 | Train loss: 0.397 | Val loss: 0.962 | Gen: ethay ariray onditingcay isway okwingway\n",
            "Epoch:  38 | Train loss: 0.378 | Val loss: 0.992 | Gen: ethay ariway onditingway isway okwingway\n",
            "Epoch:  39 | Train loss: 0.378 | Val loss: 0.977 | Gen: ethay ariray onditingcay isway okwingway\n",
            "Epoch:  40 | Train loss: 0.364 | Val loss: 1.008 | Gen: ethay ariway onditingwationway isway okwingway\n",
            "Epoch:  41 | Train loss: 0.365 | Val loss: 1.001 | Gen: ethay ariway onditingcay isway okwingway\n",
            "Epoch:  42 | Train loss: 0.346 | Val loss: 0.992 | Gen: ethay ariway onditingcay-ontitina isway okwingway\n",
            "Epoch:  43 | Train loss: 0.346 | Val loss: 1.048 | Gen: ethay ariway onditingcay isway okwingway\n",
            "Epoch:  44 | Train loss: 0.362 | Val loss: 1.036 | Gen: ethay ariway onditingcay isway okwingway\n",
            "Epoch:  45 | Train loss: 0.373 | Val loss: 1.091 | Gen: ethay arway ondidincay-ondingiti isway okwingway\n",
            "Epoch:  46 | Train loss: 0.376 | Val loss: 1.090 | Gen: ethay arway onditingnay isway orkwingway\n",
            "Epoch:  47 | Train loss: 0.383 | Val loss: 1.120 | Gen: ethay arway ondidincatingway isway okwingway\n",
            "Epoch:  48 | Train loss: 0.351 | Val loss: 1.004 | Gen: ethay arway onditingcay-ingcay isway okwingway\n",
            "Epoch:  49 | Train loss: 0.306 | Val loss: 0.975 | Gen: ethay arway onditingcay isway okwingway\n",
            "Epoch:  50 | Train loss: 0.286 | Val loss: 0.979 | Gen: ethay arway onditingcay isway okwingway\n",
            "Epoch:  51 | Train loss: 0.280 | Val loss: 0.965 | Gen: ethay arway onditingcay isway okwingway\n",
            "Epoch:  52 | Train loss: 0.271 | Val loss: 0.955 | Gen: ethay arway onditingcay isway okwingway\n",
            "Epoch:  53 | Train loss: 0.261 | Val loss: 0.958 | Gen: ethay arway ondidinctingway isway okwingway\n",
            "Epoch:  54 | Train loss: 0.253 | Val loss: 0.949 | Gen: ethay ariway onditincay-ingcay isway okwingway\n",
            "Epoch:  55 | Train loss: 0.243 | Val loss: 0.960 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  56 | Train loss: 0.237 | Val loss: 0.965 | Gen: ethay ariway onditincay-ingcay isway okwingway\n",
            "Epoch:  57 | Train loss: 0.229 | Val loss: 0.971 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  58 | Train loss: 0.224 | Val loss: 0.978 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  59 | Train loss: 0.217 | Val loss: 0.991 | Gen: ethay ariway onditincingnay isway okwingway\n",
            "Epoch:  60 | Train loss: 0.215 | Val loss: 0.975 | Gen: ethay ariway onditioningcay isway okwingway\n",
            "Epoch:  61 | Train loss: 0.209 | Val loss: 0.974 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  62 | Train loss: 0.202 | Val loss: 0.998 | Gen: ethay ariway ondidincingnay isway okwingway\n",
            "Epoch:  63 | Train loss: 0.197 | Val loss: 1.003 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  64 | Train loss: 0.195 | Val loss: 1.010 | Gen: ethay ariway onditioningcay isway okwingway\n",
            "Epoch:  65 | Train loss: 0.190 | Val loss: 1.047 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  66 | Train loss: 0.202 | Val loss: 1.030 | Gen: ethay ariway ondidincingnay isway okwingway\n",
            "Epoch:  67 | Train loss: 0.188 | Val loss: 1.045 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  68 | Train loss: 0.178 | Val loss: 1.016 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  69 | Train loss: 0.171 | Val loss: 1.055 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  70 | Train loss: 0.165 | Val loss: 1.079 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  71 | Train loss: 0.159 | Val loss: 1.081 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  72 | Train loss: 0.155 | Val loss: 1.113 | Gen: ethay ariway ondidionctingnay isway okwingway\n",
            "Epoch:  73 | Train loss: 0.152 | Val loss: 1.129 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  74 | Train loss: 0.149 | Val loss: 1.115 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  75 | Train loss: 0.146 | Val loss: 1.201 | Gen: ethay ariway ondidinctingnay isway okwingway\n",
            "Epoch:  76 | Train loss: 0.150 | Val loss: 1.068 | Gen: ethay ariway onditioningnay isway okwingway\n",
            "Epoch:  77 | Train loss: 0.198 | Val loss: 1.308 | Gen: ethay airay ondidiongntingnay isway okwingway\n",
            "Epoch:  78 | Train loss: 0.243 | Val loss: 1.031 | Gen: ethay arirway onditincingnay isway okingingway\n",
            "Epoch:  79 | Train loss: 0.208 | Val loss: 0.982 | Gen: ethay ariway ondidioningcay isway okwingway\n",
            "Epoch:  80 | Train loss: 0.211 | Val loss: 1.404 | Gen: ethay arfay odddincatingway isway okingway\n",
            "Epoch:  81 | Train loss: 0.269 | Val loss: 1.033 | Gen: ethay ariway onditingningnay isway okwingway\n",
            "Epoch:  82 | Train loss: 0.190 | Val loss: 0.889 | Gen: ethay ariway onditingningnay isway okwingway\n",
            "Epoch:  83 | Train loss: 0.150 | Val loss: 0.883 | Gen: ethay ariway onditingningncay isway okwingway\n",
            "Epoch:  84 | Train loss: 0.138 | Val loss: 0.903 | Gen: ethay ariway onditingningncay isway okwingnay\n",
            "Epoch:  85 | Train loss: 0.130 | Val loss: 0.919 | Gen: ethay ariway onditingningncay isway okwingnay\n",
            "Epoch:  86 | Train loss: 0.125 | Val loss: 0.922 | Gen: ethay ariway onditioningncay isway okwingnay\n",
            "Epoch:  87 | Train loss: 0.121 | Val loss: 0.925 | Gen: ethay ariway onditioningncay isway okwingnay\n",
            "Epoch:  88 | Train loss: 0.117 | Val loss: 0.926 | Gen: ethay ariway onditioningncay isway okwingway\n",
            "Epoch:  89 | Train loss: 0.113 | Val loss: 0.928 | Gen: ethay ariway onditioningncay isway okwingway\n",
            "Epoch:  90 | Train loss: 0.110 | Val loss: 0.929 | Gen: ethay ariway onditioningncay isway okwingway\n",
            "Epoch:  91 | Train loss: 0.107 | Val loss: 0.931 | Gen: ethay ariway onditioningncay isway okwingway\n",
            "Epoch:  92 | Train loss: 0.104 | Val loss: 0.935 | Gen: ethay ariway onditioningncay isway okwingway\n",
            "Epoch:  93 | Train loss: 0.101 | Val loss: 0.937 | Gen: ethay ariway onditioningncay isway okwingway\n",
            "Epoch:  94 | Train loss: 0.099 | Val loss: 0.940 | Gen: ethay ariway onditioningncay isway okwingway\n",
            "Epoch:  95 | Train loss: 0.096 | Val loss: 0.943 | Gen: ethay ariway onditioningncay isway okwingway\n",
            "Epoch:  96 | Train loss: 0.094 | Val loss: 0.946 | Gen: ethay ariway onditioningncay isway okwingway\n",
            "Epoch:  97 | Train loss: 0.091 | Val loss: 0.950 | Gen: ethay ariway onditioningncay isway okwingway\n",
            "Epoch:  98 | Train loss: 0.089 | Val loss: 0.953 | Gen: ethay ariway onditioningncay isway okwingway\n",
            "Epoch:  99 | Train loss: 0.087 | Val loss: 0.957 | Gen: ethay ariway onditioningncay isway okwingway\n",
            "Obtained lowest validation loss of: 0.8828934542834759\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway onditioningncay isway okwingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l28mKuZxvaRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135ff2bd-7d45-4ba6-9cad-c25de87be80e"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway onditioningncay isway okwingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8EqLYFu48H"
      },
      "source": [
        "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdZO69DozuUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfb655d0-cde9-4240-9e10-ad46c181b8bc"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans32_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large', # Increased data set size\n",
        "              'cuda':True, \n",
        "              'nepochs':100,\n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 10,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 512,\n",
        "              'hidden_size': 32,\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans32_args_l.update(args_dict)\n",
        "print_opts(trans32_args_l)\n",
        "\n",
        "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 10                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('initiation', 'initiationway')\n",
            "('inquisitiveness', 'inquisitivenessway')\n",
            "('quiz', 'uizqay')\n",
            "('ata', 'ataway')\n",
            "('clock', 'ockclay')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.973 | Val loss: 2.398 | Gen: ewewey ay atnonntntnnnnnnnnnln iwly ontay\n",
            "Epoch:   1 | Train loss: 2.149 | Val loss: 2.120 | Gen: edey imay intntntntnly iy ontintintingtintay\n",
            "Epoch:   2 | Train loss: 1.914 | Val loss: 1.970 | Gen: eday imay ontntntntnnngsssssss iyssssssssssssssssss ontintintintintintin\n",
            "Epoch:   3 | Train loss: 1.754 | Val loss: 1.861 | Gen: eday imay inonway iwsssssssssssssssssw ontintintintintinswa\n",
            "Epoch:   4 | Train loss: 1.624 | Val loss: 1.781 | Gen: eway iway ingingway iwssssssssssssssssws ontingstintay\n",
            "Epoch:   5 | Train loss: 1.513 | Val loss: 1.696 | Gen: eway irayway inginginginginginay iwssswsssssway orntingway\n",
            "Epoch:   6 | Train loss: 1.426 | Val loss: 1.644 | Gen: eway imway ingingingingwangindw iwsssway ornway-indsway\n",
            "Epoch:   7 | Train loss: 1.352 | Val loss: 1.621 | Gen: eway iraywaway oninginginginginandi iwswswswsway orway-indway\n",
            "Epoch:   8 | Train loss: 1.303 | Val loss: 1.537 | Gen: eway iraway ooooioniongiongionay iwsway orway\n",
            "Epoch:   9 | Train loss: 1.248 | Val loss: 1.730 | Gen: ehay irawaway iongiongiongionangio iway oroiooonday\n",
            "Epoch:  10 | Train loss: 1.196 | Val loss: 1.529 | Gen: eway iray onioionginay iway orway-indway\n",
            "Epoch:  11 | Train loss: 1.123 | Val loss: 1.424 | Gen: eheway iray oniooiongingionay iway orway-ionway\n",
            "Epoch:  12 | Train loss: 1.076 | Val loss: 1.352 | Gen: ethay iraway oniongionay iway orway\n",
            "Epoch:  13 | Train loss: 1.022 | Val loss: 1.376 | Gen: ehethay iraway ongiongiongionay iway orkiongway\n",
            "Epoch:  14 | Train loss: 0.997 | Val loss: 1.373 | Gen: ethay iraway oniongionangionginay iway orway\n",
            "Epoch:  15 | Train loss: 0.946 | Val loss: 1.301 | Gen: ehay irray ongionginangiongway iway oringway\n",
            "Epoch:  16 | Train loss: 0.917 | Val loss: 1.362 | Gen: ehay iraway ongionginangway iway orway\n",
            "Epoch:  17 | Train loss: 0.893 | Val loss: 1.247 | Gen: ehway irray ongiongingionay iway oringway\n",
            "Epoch:  18 | Train loss: 0.884 | Val loss: 1.209 | Gen: ehay irray ondicatingiongway iway oringway\n",
            "Epoch:  19 | Train loss: 0.831 | Val loss: 1.179 | Gen: ehway irray ongiongiongway iway oringway\n",
            "Epoch:  20 | Train loss: 0.807 | Val loss: 1.212 | Gen: ehay irray ongiongwaongway iway oringway\n",
            "Epoch:  21 | Train loss: 0.770 | Val loss: 1.187 | Gen: ehhay irray ondiongionay iway oringway\n",
            "Epoch:  22 | Train loss: 0.751 | Val loss: 1.172 | Gen: ehay irray ondiongionay iway oringway\n",
            "Epoch:  23 | Train loss: 0.725 | Val loss: 1.154 | Gen: ehay irray ondiongionay isway oringway\n",
            "Epoch:  24 | Train loss: 0.706 | Val loss: 1.141 | Gen: ehay iray ondionionay iway oringway\n",
            "Epoch:  25 | Train loss: 0.690 | Val loss: 1.127 | Gen: ehthay array ondiongionay iway oringway\n",
            "Epoch:  26 | Train loss: 0.659 | Val loss: 1.077 | Gen: ehay irray ondionionguionay isway oringway\n",
            "Epoch:  27 | Train loss: 0.633 | Val loss: 1.065 | Gen: ehay irray ondiongionay iway oringway\n",
            "Epoch:  28 | Train loss: 0.615 | Val loss: 1.036 | Gen: ehay airay ondioniongway iway oringway\n",
            "Epoch:  29 | Train loss: 0.593 | Val loss: 1.043 | Gen: ehway irray ondioniongway isway oringway\n",
            "Epoch:  30 | Train loss: 0.582 | Val loss: 1.024 | Gen: ehay airay ondioniongway iway oringway\n",
            "Epoch:  31 | Train loss: 0.563 | Val loss: 1.000 | Gen: ehway irray onditiongway iway oringway\n",
            "Epoch:  32 | Train loss: 0.551 | Val loss: 0.978 | Gen: ehay iray ondioniongway isway oringway\n",
            "Epoch:  33 | Train loss: 0.532 | Val loss: 0.986 | Gen: ehway irray onditiongway isway oringway\n",
            "Epoch:  34 | Train loss: 0.522 | Val loss: 0.978 | Gen: ehay iray onditiongway iway oringway\n",
            "Epoch:  35 | Train loss: 0.504 | Val loss: 0.974 | Gen: ehway irray onditionginay iway oringway\n",
            "Epoch:  36 | Train loss: 0.489 | Val loss: 0.959 | Gen: ehay iray onditiongway iway oringway\n",
            "Epoch:  37 | Train loss: 0.483 | Val loss: 1.085 | Gen: ehway iraway ondioningway iway oringway\n",
            "Epoch:  38 | Train loss: 0.501 | Val loss: 0.941 | Gen: ehay airay onditiongiongway isway oringway\n",
            "Epoch:  39 | Train loss: 0.470 | Val loss: 0.926 | Gen: ehway irray onditiongiongway isway oringway\n",
            "Epoch:  40 | Train loss: 0.441 | Val loss: 0.908 | Gen: ehtay iray onditiongcay iway oringway\n",
            "Epoch:  41 | Train loss: 0.435 | Val loss: 0.858 | Gen: ehtay irray onditiongingway isway oringway\n",
            "Epoch:  42 | Train loss: 0.416 | Val loss: 0.873 | Gen: ehtay iray onditiongingway isway oringway\n",
            "Epoch:  43 | Train loss: 0.410 | Val loss: 0.846 | Gen: ehtay irray onditiongcangway iway oringway\n",
            "Epoch:  44 | Train loss: 0.406 | Val loss: 0.868 | Gen: ehtay irray onditiongingway isway oringway\n",
            "Epoch:  45 | Train loss: 0.386 | Val loss: 0.835 | Gen: ehtay iraway onditiongingway isway oringway\n",
            "Epoch:  46 | Train loss: 0.373 | Val loss: 0.832 | Gen: ehtay irray onditiongingway isway oringway\n",
            "Epoch:  47 | Train loss: 0.367 | Val loss: 0.822 | Gen: ehhay irray onditiongcangway isway oringway\n",
            "Epoch:  48 | Train loss: 0.360 | Val loss: 0.820 | Gen: ehtay array onditiongcangway isway oringway\n",
            "Epoch:  49 | Train loss: 0.352 | Val loss: 0.796 | Gen: ehtay irray onditiongcangway isway orkingway\n",
            "Epoch:  50 | Train loss: 0.341 | Val loss: 1.041 | Gen: ehtay irray onditiongingway isway orinngway\n",
            "Epoch:  51 | Train loss: 0.459 | Val loss: 0.899 | Gen: ehway airway ondizaingiongway iway oringway\n",
            "Epoch:  52 | Train loss: 0.390 | Val loss: 0.772 | Gen: ehtay irray onditiongingway isway oringway\n",
            "Epoch:  53 | Train loss: 0.337 | Val loss: 0.766 | Gen: ehtay irray onditiongcay isway oringway\n",
            "Epoch:  54 | Train loss: 0.320 | Val loss: 0.751 | Gen: ehtay array onditiongcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.307 | Val loss: 0.746 | Gen: ehtay arrway onditiongcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.299 | Val loss: 0.743 | Gen: ehtay arrway onditiongcangway isway orkingway\n",
            "Epoch:  57 | Train loss: 0.292 | Val loss: 0.743 | Gen: ehtay arrway onditiongcangway isway orkingway\n",
            "Epoch:  58 | Train loss: 0.285 | Val loss: 0.740 | Gen: ehtay arrway onditiongcangway isway orkingway\n",
            "Epoch:  59 | Train loss: 0.279 | Val loss: 0.741 | Gen: ehtay arrway onditiongcangway isway orkingway\n",
            "Epoch:  60 | Train loss: 0.273 | Val loss: 0.740 | Gen: ehtay arrway onditiongcangway isway orkingway\n",
            "Epoch:  61 | Train loss: 0.269 | Val loss: 0.743 | Gen: ehtay arrway onditiongcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.282 | Val loss: 0.816 | Gen: hetway arrway onditiongcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.349 | Val loss: 0.823 | Gen: ethay arrway onditiongcangway isway orkingway\n",
            "Epoch:  64 | Train loss: 0.302 | Val loss: 0.786 | Gen: ehthay irray onditiongingway isway orkingway\n",
            "Epoch:  65 | Train loss: 0.291 | Val loss: 0.727 | Gen: ehtay irway onditiongingway isway oringway\n",
            "Epoch:  66 | Train loss: 0.258 | Val loss: 0.734 | Gen: ehthay arrway onditiongingway isway orkingway\n",
            "Epoch:  67 | Train loss: 0.250 | Val loss: 0.716 | Gen: ehtay arrway onditiongingway isway orkingway\n",
            "Epoch:  68 | Train loss: 0.241 | Val loss: 0.721 | Gen: ehtay arrway onditiongingway isway orkingway\n",
            "Epoch:  69 | Train loss: 0.237 | Val loss: 0.711 | Gen: ehtay arrway onditiongcangway isway orkingway\n",
            "Epoch:  70 | Train loss: 0.231 | Val loss: 0.717 | Gen: ehtay arrway onditiongingway isway orkingway\n",
            "Epoch:  71 | Train loss: 0.227 | Val loss: 0.708 | Gen: ehtay arrway onditiongcangway isway orkingway\n",
            "Epoch:  72 | Train loss: 0.221 | Val loss: 0.713 | Gen: ehtay arrway onditiongcangway isway orkingway\n",
            "Epoch:  73 | Train loss: 0.217 | Val loss: 0.709 | Gen: ehtay arrway onditiongcangway isway orkingway\n",
            "Epoch:  74 | Train loss: 0.212 | Val loss: 0.709 | Gen: ehtay arrway onditiongcangway isway orkingway\n",
            "Epoch:  75 | Train loss: 0.208 | Val loss: 0.709 | Gen: ethay arrway onditiongcangway isway orkingway\n",
            "Epoch:  76 | Train loss: 0.204 | Val loss: 0.708 | Gen: ethay arrway onditiongcangway isway orkingway\n",
            "Epoch:  77 | Train loss: 0.200 | Val loss: 0.709 | Gen: ethay arrway onditiongcangway isway orkingway\n",
            "Epoch:  78 | Train loss: 0.196 | Val loss: 0.708 | Gen: ethay arrway onditiongcangway isway orkingway\n",
            "Epoch:  79 | Train loss: 0.193 | Val loss: 0.711 | Gen: ethay arrway onditiongcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.189 | Val loss: 0.715 | Gen: ethay arrway onditiongcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.186 | Val loss: 0.714 | Gen: ethay arrway onditiongcay isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.7077494359352082\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay arrway onditiongcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51645927-57b0-4de6-d7de-2bb1d83868ea"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans64_args_s = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_small',\n",
        "              'cuda':True, \n",
        "              'nepochs':50, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 20,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 64, \n",
        "              'hidden_size': 64, # Increased model size\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans64_args_s.update(args_dict)\n",
        "print_opts(trans64_args_s)\n",
        "\n",
        "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('applying', 'applyingway')\n",
            "('luxury', 'uxurylay')\n",
            "('discovering', 'iscoveringday')\n",
            "('gratefully', 'atefullygray')\n",
            "('enjoy', 'enjoyway')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.592 | Val loss: 2.022 | Gen: ay ay indonay-ay-ay-ay-ay- isway isway-ay-ay-e-ay\n",
            "Epoch:   1 | Train loss: 1.764 | Val loss: 1.704 | Gen: ehay iray indinay-ingndgndgay- isway ingway-ingway-iggggg\n",
            "Epoch:   2 | Train loss: 1.496 | Val loss: 1.590 | Gen: ethay ay indingay-indndndnay- isway ay-igggay-igggggggay\n",
            "Epoch:   3 | Train loss: 1.279 | Val loss: 1.525 | Gen: ethay arrrway indngay-indnay-indna isway kgay-y-ygay\n",
            "Epoch:   4 | Train loss: 1.150 | Val loss: 1.404 | Gen: eway araray odingay-ingingdinay- isay kway-ay-ay\n",
            "Epoch:   5 | Train loss: 1.017 | Val loss: 1.271 | Gen: ehay arirway indingingcay isway irkway-ingway\n",
            "Epoch:   6 | Train loss: 0.886 | Val loss: 1.128 | Gen: ehay arway odingay-ingiondingin isway orkway-ingway\n",
            "Epoch:   7 | Train loss: 0.754 | Val loss: 1.044 | Gen: ehay arway odingitingay isway irkway-ingway\n",
            "Epoch:   8 | Train loss: 0.641 | Val loss: 0.937 | Gen: ethay arway ondiongay isway kkkway-ingway\n",
            "Epoch:   9 | Train loss: 0.565 | Val loss: 0.993 | Gen: ehay arway odintitingcay isway orkway-ay\n",
            "Epoch:  10 | Train loss: 0.538 | Val loss: 1.137 | Gen: eththththay arway ondititingcongciongc isway orkingway\n",
            "Epoch:  11 | Train loss: 0.487 | Val loss: 0.856 | Gen: ethay arway ondititingcay isway orway-ingway\n",
            "Epoch:  12 | Train loss: 0.416 | Val loss: 0.786 | Gen: ehay arrway onditingcay-ongday isway orkingway\n",
            "Epoch:  13 | Train loss: 0.345 | Val loss: 0.922 | Gen: ethay arrway ondiontingongcay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.315 | Val loss: 0.899 | Gen: thay arway onditingcay-ongcay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.343 | Val loss: 0.954 | Gen: htethay arrway onditiongcay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.339 | Val loss: 0.771 | Gen: ehay arrway odinticay isway orkingway\n",
            "Epoch:  17 | Train loss: 0.264 | Val loss: 0.733 | Gen: ethay arrway onditingcay-ontingca isway orkingway\n",
            "Epoch:  18 | Train loss: 0.207 | Val loss: 0.771 | Gen: ehethay arrway onditiningcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.193 | Val loss: 0.864 | Gen: ehay aarway onditiongcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.208 | Val loss: 0.874 | Gen: thay arrway odnitioningcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.168 | Val loss: 0.655 | Gen: ethay arrway onditiningcay isway owringway\n",
            "Epoch:  22 | Train loss: 0.127 | Val loss: 0.656 | Gen: ethay arrway onditioningcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.115 | Val loss: 0.693 | Gen: ethay arrway onditiongcay isway owringway\n",
            "Epoch:  24 | Train loss: 0.107 | Val loss: 0.757 | Gen: ehthay arrway onditiongcay isway owingway\n",
            "Epoch:  25 | Train loss: 0.107 | Val loss: 0.686 | Gen: ethay aarrway onditionicay isway owringway\n",
            "Epoch:  26 | Train loss: 0.137 | Val loss: 1.149 | Gen: eewhay arrway ondiditingcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.294 | Val loss: 1.006 | Gen: ethay arway oodititingway isway oway-ingway\n",
            "Epoch:  28 | Train loss: 0.240 | Val loss: 0.850 | Gen: ethay iarrway odnitiningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.164 | Val loss: 0.706 | Gen: ththay arrway onditiningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.118 | Val loss: 0.579 | Gen: ethay arrway onditiongcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.082 | Val loss: 0.604 | Gen: ethay arrway onditincingcay isway owkingway\n",
            "Epoch:  32 | Train loss: 0.067 | Val loss: 0.567 | Gen: ethay arrway onditincingcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.054 | Val loss: 0.587 | Gen: ethay arrway onditiongcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.041 | Val loss: 0.580 | Gen: ethay arrway onditiongcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.034 | Val loss: 0.590 | Gen: ethay arrway onditiningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.029 | Val loss: 0.568 | Gen: ethay arrway onditiningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.026 | Val loss: 0.624 | Gen: ethay arrway onditiongcay isway okingway\n",
            "Epoch:  38 | Train loss: 0.024 | Val loss: 0.576 | Gen: ethay arrway onditiongcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.021 | Val loss: 0.590 | Gen: ethay arrway onditiningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.018 | Val loss: 0.601 | Gen: ethay arrway onditiongcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.016 | Val loss: 0.658 | Gen: ethay arway onditiongcay isway okingngway\n",
            "Epoch:  42 | Train loss: 0.015 | Val loss: 0.626 | Gen: ethay arrway onditiongcay isway okingway\n",
            "Epoch:  43 | Train loss: 0.018 | Val loss: 0.628 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.066 | Val loss: 1.209 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.251 | Val loss: 0.804 | Gen: ethay irway onditiongcingay isway orwingway\n",
            "Epoch:  46 | Train loss: 0.119 | Val loss: 0.647 | Gen: ethay arway onditioniiongcay iway okingway\n",
            "Epoch:  47 | Train loss: 0.097 | Val loss: 0.638 | Gen: ethay arway onditiongcay isway okingway\n",
            "Epoch:  48 | Train loss: 0.071 | Val loss: 0.589 | Gen: ethay ariway onditiningcay isway okingway\n",
            "Epoch:  49 | Train loss: 0.039 | Val loss: 0.568 | Gen: ethay ariway onditioningcay isway okingway\n",
            "Obtained lowest validation loss of: 0.5669146737253125\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway onditioningcay isway okingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dardK4RWvUWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254bab6e-d75e-4567-ab3a-dc59c771a757"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "trans64_args_l = AttrDict()\n",
        "args_dict = {\n",
        "              'data_file_name': 'pig_latin_large', # Increased data set size\n",
        "              'cuda':True, \n",
        "              'nepochs':50,\n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':5e-4,\n",
        "              'early_stopping_patience': 20,\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size': 512, \n",
        "              'hidden_size': 64, # Increased model size\n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "trans64_args_l.update(args_dict)\n",
        "print_opts(trans64_args_l)\n",
        "\n",
        "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('initiation', 'initiationway')\n",
            "('inquisitiveness', 'inquisitivenessway')\n",
            "('quiz', 'uizqay')\n",
            "('ata', 'ataway')\n",
            "('clock', 'ockclay')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.477 | Val loss: 2.097 | Gen: ay irrray innnnnnnnnnnnnnnnnnn iway innnnnnnnnnnway\n",
            "Epoch:   1 | Train loss: 1.740 | Val loss: 1.896 | Gen: ay ayway intintintintinay-ind wsway irgrgay-ingay\n",
            "Epoch:   2 | Train loss: 1.508 | Val loss: 1.654 | Gen: etay iway indintinay isway ingway\n",
            "Epoch:   3 | Train loss: 1.353 | Val loss: 1.594 | Gen: etay away-iway oninay-intinaway isway oray-ingay\n",
            "Epoch:   4 | Train loss: 1.211 | Val loss: 1.700 | Gen: ethay airaray-iway iningcay-inincay isway ingigray-ininway\n",
            "Epoch:   5 | Train loss: 1.138 | Val loss: 1.514 | Gen: etey aiway onininay-ininininay isway orininay\n",
            "Epoch:   6 | Train loss: 1.001 | Val loss: 1.457 | Gen: etay irararay otingcay-inginay issway oringay-ingay\n",
            "Epoch:   7 | Train loss: 0.931 | Val loss: 1.364 | Gen: ethway arway ondontinnnnnnay sway ordwargngray\n",
            "Epoch:   8 | Train loss: 0.798 | Val loss: 1.272 | Gen: ethay irarway ondingnctinnnway isway oringrngway\n",
            "Epoch:   9 | Train loss: 0.731 | Val loss: 1.308 | Gen: ethay irarway ondiningtinngway issiway oringkgway\n",
            "Epoch:  10 | Train loss: 0.666 | Val loss: 1.014 | Gen: ehway irway oningtingongingay isway orgingway\n",
            "Epoch:  11 | Train loss: 0.606 | Val loss: 1.042 | Gen: ethway iraway oninictingingcay isway orkingingway\n",
            "Epoch:  12 | Train loss: 0.551 | Val loss: 0.862 | Gen: ethay ariway ondiotiongcay-onay isway okingingay\n",
            "Epoch:  13 | Train loss: 0.482 | Val loss: 1.002 | Gen: ehehay airway ondidictionginay isway okingingway\n",
            "Epoch:  14 | Train loss: 0.459 | Val loss: 0.820 | Gen: thay arirwwwwway onditingcay-ingcay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.450 | Val loss: 0.790 | Gen: ethay arirway onitiotiongiongay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.408 | Val loss: 0.977 | Gen: ehway airwwwwway ondidintingingday isway owkingbway\n",
            "Epoch:  17 | Train loss: 0.373 | Val loss: 0.877 | Gen: ehay arirway ondintinngngway isway orkingway\n",
            "Epoch:  18 | Train loss: 0.345 | Val loss: 0.794 | Gen: etay arway onditinongay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.295 | Val loss: 0.737 | Gen: ethay airway ondidtinngngcay issway okingrngway\n",
            "Epoch:  20 | Train loss: 0.253 | Val loss: 0.655 | Gen: etay arway onditiongcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.219 | Val loss: 0.596 | Gen: ethay ariway onditiongngingay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.191 | Val loss: 0.643 | Gen: ethay arwwway onditiongingay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.189 | Val loss: 0.756 | Gen: thehay arrway inditiongingway isway orkingway\n",
            "Epoch:  24 | Train loss: 0.211 | Val loss: 0.684 | Gen: ethay arwwway onditiongcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.203 | Val loss: 0.628 | Gen: ethay ariway onditiongngngway isway orkingway\n",
            "Epoch:  26 | Train loss: 0.176 | Val loss: 0.572 | Gen: ethay arway onditiongnay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.141 | Val loss: 0.504 | Gen: ethay arrway onditiongingway isway orkingway\n",
            "Epoch:  28 | Train loss: 0.124 | Val loss: 0.485 | Gen: ethay arway onditiongingcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.120 | Val loss: 0.502 | Gen: ethay ariway onditiongiongcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.119 | Val loss: 0.439 | Gen: ethay arway onditiongingcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.110 | Val loss: 0.432 | Gen: ethay ariway onditiongingcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.092 | Val loss: 0.852 | Gen: tehay ariway oditiontingingcay isway okingwray\n",
            "Epoch:  33 | Train loss: 0.325 | Val loss: 0.801 | Gen: thehay awwwway onditioningcay isway owkingway\n",
            "Epoch:  34 | Train loss: 0.232 | Val loss: 0.600 | Gen: ethay awarwway onditioncay-ongway isway orkingway\n",
            "Epoch:  35 | Train loss: 0.159 | Val loss: 0.380 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.103 | Val loss: 0.361 | Gen: ethay airway onditioningngcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.079 | Val loss: 0.320 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.062 | Val loss: 0.299 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.052 | Val loss: 0.313 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.048 | Val loss: 0.281 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.044 | Val loss: 0.353 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.072 | Val loss: 0.322 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.067 | Val loss: 0.569 | Gen: -ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.080 | Val loss: 0.342 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.044 | Val loss: 0.318 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.039 | Val loss: 0.319 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.048 | Val loss: 0.322 | Gen: ehthay airway onditioniongcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.039 | Val loss: 0.299 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.031 | Val loss: 0.371 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.28121999498762307\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSyiG39vVlN"
      },
      "source": [
        "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ql0pxrEvVP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "065cef58-ecb0-4cf9-b4f4-7c153648c177"
      },
      "source": [
        "save_loss_comparison_by_dataset(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_dataset')\n",
        "save_loss_comparison_by_hidden(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_hidden')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBnBXRG8mvcn"
      },
      "source": [
        "# Optional: Attention Visualizations\n",
        "\n",
        "One of the benefits of using attention is that it allows us to gain insight into the inner workings of the model.\n",
        "\n",
        "By visualizing the attention weights generated for the input tokens in each decoder step, we can see where the model focuses while producing each output token.\n",
        "\n",
        "The code in this section loads the model you trained from the previous section and uses it to translate a given set of words: it prints the translations and display heatmaps to show how attention is used at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEC0vN9mvpV"
      },
      "source": [
        "## Step 1: Visualize Attention Masks\n",
        "Play around with visualizing attention maps generated by the previous two models you've trained. Inspect visualizations in one success and one failure case for both models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkfz-u-MtudL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "16c3ca5f-8154-4c9e-e38b-f23cee556dc8"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'eettsray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssa7g35zt2yj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "dc57fdab-9c1e-4faa-86d9-e1b6aab60717"
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'eetstray'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}